{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "source": [
    "# Visualization of CNN: Grad-CAM\n",
    "## By : Bazaz Samuel\n",
    "\n",
    "* **Objective**: Convolutional Neural Networks are widely used on computer vision. It is powerful for processing grid-like data. However we hardly know how and why it works, due to the lack of decomposability into individually intuitive components. In this assignment, we use Grad-CAM, which highlights the regions of the input image that were important for the neural network prediction.\n",
    "\n",
    "\n",
    "* NB: if `PIL` is not installed, try `conda install pillow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "source": [
    "### Download the Model\n",
    "We provide you a pretrained model `ResNet-34` for `ImageNet` classification dataset.\n",
    "* **ImageNet**: A large dataset of photographs with 1 000 classes.\n",
    "* **ResNet-34**: A deep architecture for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "outputs": [],
   "source": [
    "resnet34 = models.resnet34(weights='ResNet34_Weights.IMAGENET1K_V1')  # New PyTorch interface for loading weights!\n",
    "resnet34.eval() # set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "source": [
    "![ResNet34](https://miro.medium.com/max/1050/1*Y-u7dH4WC-dXyn9jOG4w0w.png)\n",
    "\n",
    "\n",
    "Input image must be of size (3x224x224). \n",
    "\n",
    "First convolution layer with maxpool. \n",
    "Then 4 ResNet blocks. \n",
    "\n",
    "Output of the last ResNet block is of size (512x7x7). \n",
    "\n",
    "Average pooling is applied to this layer to have a 1D array of 512 features fed to a linear layer that outputs 1000 values (one for each class). No softmax is present in this case. We have already the raw class score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "outputs": [],
   "source": [
    "classes = pickle.load(urllib.request.urlopen('https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'))\n",
    "\n",
    "##classes is a dictionary with the name of each class \n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "source": [
    "### Input Images\n",
    "We provide you 20 images from ImageNet (download link on the webpage of the course or download directly using the following command line,).<br>\n",
    "In order to use the pretrained model resnet34, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(224, 224)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(dir_path):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    # Note: If the inverse normalisation is required, apply 1/x to the above object\n",
    "    \n",
    "    dataset = datasets.ImageFolder(dir_path, transforms.Compose([\n",
    "            transforms.Resize(256), \n",
    "            transforms.CenterCrop(224), # resize the image to 224x224\n",
    "            transforms.ToTensor(), # convert numpy.array to tensor\n",
    "            normalize])) #normalize the tensor\n",
    "\n",
    "    return (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "if not os.path.exists(\"data/TP2_images\"):\n",
    "    os.mkdir(\"data/TP2_images\")\n",
    "    !cd data/TP2_images && wget \"https://www.lri.fr/~gcharpia/deeppractice/2023/TP2/TP2_images.zip\" && unzip TP2_images.zip\n",
    "\n",
    "dir_path = \"data/\" \n",
    "dataset = preprocess_image(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "outputs": [],
   "source": [
    "# show the orignal image \n",
    "index = 5\n",
    "input_image = Image.open(dataset.imgs[index][0]).convert('RGB')\n",
    "plt.imshow(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "outputs": [],
   "source": [
    "output = resnet34(dataset[index][0].view(1, 3, 224, 224))\n",
    "values, indices = torch.topk(output, 3)\n",
    "print(\"Top 3-classes:\", indices[0].numpy(), [classes[x] for x in indices[0].numpy()])\n",
    "print(\"Raw class scores:\", values[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "source": [
    "### Grad-CAM \n",
    "* **Overview:** Given an image, and a category (‘tiger cat’) as input, we forward-propagate the image through the model to obtain the `raw class scores` before softmax. The gradients are set to zero for all classes except the desired class (tiger cat), which is set to 1. This signal is then backpropagated to the `rectified convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap).\n",
    "\n",
    "\n",
    "* **To Do**: Define your own function get_grad_cam to achieve the visualization of the given images. For each image, choose the top-3 possible labels as the desired classes. Compare the heatmaps of the three classes, and conclude. \n",
    "\n",
    "\n",
    "* **To be submitted within 2 weeks**: this notebook, **cleaned** (i.e. without results, for file size reasons: `menu > kernel > restart and clean`), in a state ready to be executed (if one just presses 'Enter' till the end, one should obtain all the results for all images) with a few comments at the end. No additional report, just the notebook!\n",
    "\n",
    "\n",
    "* **Hints**: \n",
    " + We need to record the output and grad_output of the feature maps to achieve Grad-CAM. In pytorch, the function `Hook` is defined for this purpose. Read the tutorial of [hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) carefully.\n",
    " + More on [autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html) and [hooks](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks)\n",
    " + The pretrained model resnet34 doesn't have an activation function after its last layer, the output is indeed the `raw class scores`, you can use them directly. \n",
    " + The size of feature maps is 7x7, so your heatmap will have the same size. You need to project the heatmap to the resized image (224x224, not the original one, before the normalization) to have a better observation. The function [`torch.nn.functional.interpolate`](https://pytorch.org/docs/stable/nn.functional.html?highlight=interpolate#torch.nn.functional.interpolate) may help.  \n",
    " + Here is the link of the paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "source": [
    "Class: ‘pug, pug-dog’ | Class: ‘tabby, tabby cat’\n",
    "- | - \n",
    "![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dog.jpg)| ![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/cat.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "source": [
    "### Complementary questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "source": [
    "##### Try GradCAM on others convolutional layers, describe and comment the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# Fonctions #\n",
    "#############\n",
    "\n",
    "def get_grad_cam(input,category,rgb_img):\n",
    "\n",
    "  # Initialisation des accumulateurs des features et les gradients associée\n",
    "  features = []\n",
    "  gradients = []\n",
    "  # Functions de récupération des features et gradients\n",
    "  def add_to_features(model, input, output):\n",
    "    features.append(output.data)\n",
    "      \n",
    "  def add_to_gradients(model, grad_input, grad_output):\n",
    "    gradients.append(grad_output[0])\n",
    "  \n",
    "  # On lie cette récupération de données aux derniers feature maps et aux opération de forward backward.\n",
    "  # Cela permet de tirer profit des méthodes implémentés par pytorch (autograd ...).\n",
    "  features_hook = model.layer4[2].bn2.register_forward_hook(add_to_features)\n",
    "  gradients_hook = model.layer4[2].bn2.register_backward_hook(add_to_gradients)\n",
    "\n",
    "  # On fait l'étape forward\n",
    "  output = model(input)\n",
    "    \n",
    "  # On construit le tenseur indicatrice de la bonne classe pour récupérer le bon score\n",
    "  class_ind = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
    "  class_ind[0][category] = 1\n",
    "  class_ind = torch.from_numpy(class_ind).requires_grad_(True)\n",
    "  score = torch.sum(class_ind * output)\n",
    "\n",
    "  # On fait une étape de back propagation à partir de ce dernier\n",
    "  model.zero_grad()\n",
    "  score.backward(retain_graph=True) \n",
    "\n",
    "  # Suite aux étapes de forward backward nos hooks ont récupéré les données voulues \n",
    "  # On peut alors les enlever\n",
    "  gradients = gradients[0][-1].numpy()\n",
    "  features = features[0][-1].numpy()\n",
    "  gradients_hook.remove()\n",
    "  features_hook.remove()\n",
    "    \n",
    "  # les gradients sont utilisés pour mesurer la constribution de chaque feature map à la classification.\n",
    "  # L'objectif est d'ensuite utiliser cette pondération pour agréger les canaux et obtenir une unique contribution par région de l'image\n",
    "  fmps_weights = np.mean(gradients, axis=(1,2))\n",
    "  contributions = np.zeros(features.shape[1:]) \n",
    "  for i in range(fmps_weights.shape[0]): \n",
    "      contributions += fmps_weights[i] * features[i, :, :]\n",
    "\n",
    "  # Un ReLU est utilisé pour ne garder que les contributions qui améliore la classification\n",
    "  pos_contrib = np.maximum(contributions, 0)\n",
    "  \n",
    "  # Une surésolution permet ensuite de récupérer les contribution associée aux pixel initiaux\n",
    "  # en faisant l'hypothèse simplificatrice que les pixels associée à un même feature conttibuent de la même manière\n",
    "  pixel_contrib = torch.from_numpy(pos_contrib.reshape(1,1,7,7))\n",
    "  pixel_contrib = F.interpolate(pixel_contrib,scale_factor=32,mode='bilinear')\n",
    "  pixel_contrib = pixel_contrib.numpy()[0,0,:,:]\n",
    "\n",
    "  # On normalise pour avoir un code couleur identique entre chaque image\n",
    "  norm_px_cont = pixel_contrib / np.max(pixel_contrib)\n",
    "  \n",
    "  # On construit ensuite le heatmap avec la palette de l'article\n",
    "  heatmap = cv2.applyColorMap(np.uint8(255*norm_px_cont), cv2.COLORMAP_JET) \n",
    "  heatmap = np.float32(heatmap)/255\n",
    "    \n",
    "  # On construit enfin la superposition de la heatmap avec l'image originale RGB\n",
    "  # Il faut pour cela reformater un peu l'entrée\n",
    "  grad_cam = cv2.addWeighted(heatmap, 0.5, rgb_img, 0.5, 0.0)\n",
    "  grad_cam = np.uint8(255*grad_cam[:, :, ::-1])\n",
    "  return grad_cam\n",
    "\n",
    "def plot_top3_grad_cams(input,rgb_img):\n",
    "  # On visualise 4 images\n",
    "  _, ax = plt.subplots(1,4,figsize=(20,5))\n",
    "  # la première est l'image originale\n",
    "  ax[0].imshow(rgb_img)\n",
    "  ax[0].set_title(f'Sample{i+1}')\n",
    "  \n",
    "  # On évalue le modèle pour récupérer les 3 meilleures classes\n",
    "  model = models.resnet34(pretrained=True)\n",
    "  model.eval()\n",
    "  output = model(input)\n",
    "  values, indices = torch.topk(output,3)\n",
    "  \n",
    "  # Puis on récupère les grad_CAM associés\n",
    "  for j in range(1,4): \n",
    "    category = indices[0].numpy()[j-1]\n",
    "    grad_cam = get_grad_cam(input, category, rgb_img)\n",
    "    ax[j].imshow(grad_cam)\n",
    "    class_name = classes[category].split(',') \n",
    "    ax[j].set_title(class_name[0])\n",
    "  plt.show()\n",
    "\n",
    "def get_input_rgb(dataset, i):\n",
    "    # Pour construire l'image en RGB on a besoin de reformatter l'image\n",
    "    rgb_img = np.array(Image.open(dataset.imgs[i][0]).convert('RGB'))\n",
    "    rgb_img = np.float32(cv2.resize(rgb_img, (224, 224))) / 255\n",
    "    # L'input lui doit suivre le format d'entrée du model\n",
    "    input = dataset[i][0].view(1, 3, 224, 224)\n",
    "    return input, rgb_img\n",
    " \n",
    "# notre dernière fonction est alors:\n",
    "plot_top3_grad_cams_from_i = lambda i : plot_top3_grad_cams(*get_input_rgb(dataset, i))\n",
    "plot_top3_grad_cams_from_i = np.vectorize(plot_top3_grad_cams_from_i)\n",
    "\n",
    "#######\n",
    "# Run #\n",
    "#######\n",
    "plot_top3_grad_cams_from_i(np.arange(1,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au vu des résultats, on constate que le modèle est bien entrainé: l'attention est toujours porté sur les animaux et l'on n'observe pas d'erreurs grossières sauf pour l'image 20 (focalisation sur la paille) qui est caractérisée par des textures hétérogènes dans le décors et une multiplicité d'animaux disjoints. Cette moins bonne performance du grad cam pour des images avec plusieurs object/individus a d'ailleurs été relevée par les auteurs et autrices de l'article.\n",
    "Ensuite on remarque que la heatmap n'englobe pas forcément tout l'animal (occlusion), l'intérêt de cette méthode pour de la segmentation est ainsi limité.\n",
    "Enfin la forme de l'animal n'est pas respectée réellement, la heatmap est nébuleuse avec des clusters d'activation souvent convexes, un peu comme un mélange gaussien. Pour affiner les contours des régions d'activation, l'article préconise l'utilisation du guided grad CAM qui couple l'information du grad CAM avec les gradients de feature maps de plus petites granularités."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "source": [
    "##### What are the principal contributions of GradCAM (the answer is in the paper) ?\n",
    "\n",
    "L'intérêt de cette méthode est d'offrir une source de compréhension du processus de décision d'un réseau de neurone, diminuant sont caractère opaque.\n",
    "Ce gain en transparence, permet d'une part de pouvoir communiquer sur les raisons des résultats ce qui est souvent obligatoire dans nombre de milieux. C'est ce dernier point qui explique pourquoi des arbres de décisions sont toujours préférés même quand les performances sont moins bonnes.\n",
    "Ensuite l'article énumère différents utilisations du grad CAM: \n",
    "- Détecter un overfitting quand le score est correcte mais que grad CAM est abérant.\n",
    "- Identifier un \"weak learner\" et hierarchiser des modèles entre eux.\n",
    "- Identifier des biais, comme un bais sexiste : est ce que l'attention est sur les hommes quand on cherche un \"doctor\" comparé à une \"nurse\" ?\n",
    "- Détecter un problème de biais de la base d'apprentissage grâce au point précédent.\n",
    "- Identifier des caractéristiques intéressante pour notre object d'étude, surtout quand l'algorithme performe mieux que l'humain.\n",
    "\n",
    "En comparaison aux autres méthodes grad CAM généralise la méthode CAM qui est certes plus précis en terme de contours mais qui n'est compatible que lorsque la dernière couche de convolution donne sur le softmax, si j'ai bien compris. En somme grad CAM est plus général en terme d'archictecture et ne requiert par de réentrainement contrairement à des méthodes plus sophistiques mais plus couteuses, par exemple le contrastive marginal winning probability. \n",
    "\n",
    "Cette approche a permis enfin de faire différentes observations: VGG-16 est meilleurs en terme d'attention que Alex-Net et il existe une corrélation entre cette notion d'attention algorithmique et d'attention humaine pour une tache de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "1X65RIwDc37i"
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kfiletag": "1X65RIwDc37i",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
